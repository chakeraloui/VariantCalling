rule gatk_MarkDuplicates:
    input:
        bams = "aligned_reads/{sample}_unsorted.bam"
    output:
        bam = temp("aligned_reads/{sample}_unsorted_mkdups.bam"),
        metrics = "aligned_reads/{sample}_unsorted_mkdups_metrics.txt"
    params:
        maxmemory = expand('"-Xmx{maxmemory}"', maxmemory = config['MAXMEMORY']),
        tdir = config['TEMPDIR'],
        mapped_bam_readgroup="{sample}_mapped_bam",
        out="aligned_reads",
        others= " --read-validation-stringency SILENT  --optical-duplicate-pixel-distance 2500 --treat-unsorted-as-querygroup-ordered --create-output-bam-index false ",
        spark= "--conf spark.local.dir=tdir --spark-runner LOCAL --spark-master 'local[10]'   --conf spark.driver.extraJavaOptions=-Xss2m --conf spark.executor.extraJavaOptions=-Xss2m --conf 'spark.kryo.referenceTracking=false'"   
    log:
        "logs/gatk_MarkDuplicates/{sample}.log"
    benchmark:
        "benchmarks/gatk_MarkDuplicates/{sample}.tsv"
    conda:
        "../envs/gatk4.yaml"
    message:
        "Locating and tagging duplicate reads in {input}"
    shell:
        """java -Xms5000m -Xmx10000m -jar ../tools/picard.jar \
        CollectMultipleMetrics \
        --INPUT {input.bams} \
        --OUTPUT {params.out}/{params.mapped_bam_readgroup} \
        --TMP_DIR tdir \
        --POGRAM null \
        --PROGRAM CollectBaseDistributionByCycle \
        --PROGRAM CollectInsertSizeMetrics \
        --PROGRAM MeanQualityByCycle \
        --PROGRAM QualityScoreDistribution \
        --METRIC_ACCUMULATION_LEVEL null \
        --METRIC_ACCUMULATION_LEVEL ALL_READS 2>{log}_metrics && \
        gatk MarkDuplicatesSpark   -I {input.bams}   -O {output.bam} -M {output.metrics} {params.others} {params.spark}&> {log}"""

      
